# Grid Partitioned Attention: an efficient attention approximation with inductive bias for the image domain.

Code for the paper "Grid Partitioned Attention: Efficient TransformerApproximation with Inductive Bias for High Resolution Detail Generation", by Nikolay Jetchev, GÃ¶khan Yildirim, Christian Bracher, Roland Vollgraf

The file ```GPAmodule.py``` contains the GPA layer definition, and an example how to apply on an image tensor.

TODO add the full generator architecture for pose morphing with attention copying from the paper.
